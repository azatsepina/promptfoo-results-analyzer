import streamlit as st
import json
import pandas as pd
from analyze_promptfoo import PromptfooAnalyzer
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import os

# Set page configuration
st.set_page_config(
    page_title="Promptfoo Analysis Dashboard",
    page_icon="ü§ñ",
    layout="wide"
)

# Add custom CSS for better styling
st.markdown("""
    <style>
    .main {
        padding: 2rem;
    }
    .stMetric {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
    }
    .card {
        background-color: #262730;
        padding: 1rem;
        border-radius: 0.5rem;
        border: 1px solid #464855;
        margin-bottom: 1rem;
    }
    .metric-card {
        background-color: #1E1E1E;
        padding: 1.5rem;
        border-radius: 0.5rem;
        border: 1px solid #333;
        margin-bottom: 1rem;
        text-align: center;
    }
    .metric-value {
        font-size: 2rem;
        font-weight: bold;
        margin: 0.5rem 0;
    }
    .metric-label {
        font-size: 1rem;
        color: #9E9E9E;
    }
    .recommendation-high {
        border-left: 4px solid #ff4b4b;
    }
    .recommendation-medium {
        border-left: 4px solid #ffa726;
    }
    .recommendation-low {
        border-left: 4px solid #42a5f5;
    }
    </style>
""", unsafe_allow_html=True)

def create_success_rate_chart(model_performance):
    """
    Creates a bar chart showing success rates for different models
    Args:
        model_performance: Dictionary containing model performance metrics
    Returns:
        Plotly figure object
    """
    df = pd.DataFrame([
        {
            'Model': model,
            'Success Rate': perf.success_rate,  # Access as attribute
            'Cost': perf.total_cost  # Access as attribute
        }
        for model, perf in model_performance.items()
    ])
    
    fig = px.bar(
        df,
        x='Model',
        y='Success Rate',
        title='Model Success Rates',
        color='Success Rate',
        color_continuous_scale='RdYlGn'
    )
    fig.update_layout(yaxis_title='Success Rate (%)')
    return fig

def create_cost_analysis_chart(model_performance):
    """
    Creates a scatter plot comparing cost vs success rate
    Args:
        model_performance: Dictionary containing model performance metrics
    Returns:
        Plotly figure object
    """
    df = pd.DataFrame([
        {
            'Model': model,
            'Success Rate': perf.success_rate,  # Access as attribute
            'Cost': perf.total_cost,  # Access as attribute
            'Tests': perf.total_tests  # Access as attribute
        }
        for model, perf in model_performance.items()
    ])
    
    fig = px.scatter(
        df,
        x='Cost',
        y='Success Rate',
        size='Tests',
        text='Model',
        title='Cost vs Success Rate Analysis'
    )
    fig.update_traces(textposition='top center')
    fig.update_layout(
        xaxis_title='Total Cost ($)',
        yaxis_title='Success Rate (%)'
    )
    return fig

def main():
    """Main function that builds the Streamlit interface"""
    
    st.title("ü§ñ Promptfoo Analysis Dashboard")
    st.markdown("Upload your promptfoo results file to analyze performance and get insights.")
    
    uploaded_file = st.file_uploader(
        "Choose a results.json file",
        type=['json'],
        help="Upload the JSON file generated by promptfoo eval --output results.json"
    )
    
    if uploaded_file is not None:
        try:
            # Save and process file
            with open("temp_results.json", "wb") as f:
                f.write(uploaded_file.getvalue())
            
            analyzer = PromptfooAnalyzer("temp_results.json")
            model_performance = analyzer.analyze_model_performance()
            recommendations = analyzer.generate_recommendations()
            error_patterns = analyzer.analyze_error_patterns()
            
            # Key Metrics Section
            st.markdown("## üìä Key Metrics")
            total_tests = analyzer.stats['successes'] + analyzer.stats['failures']
            success_rate = (analyzer.stats['successes'] / total_tests * 100)
            total_cost = sum(result.get('cost', 0) for result in analyzer.results)
            
            # Display metrics in a row with custom styling
            metrics_html = f"""
                <div style="display: flex; justify-content: space-between; margin-bottom: 2rem;">
                    <div class="metric-card">
                        <div class="metric-label">Total Tests</div>
                        <div class="metric-value">{total_tests}</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-label">Success Rate</div>
                        <div class="metric-value">{success_rate:.1f}%</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-label">Total Cost</div>
                        <div class="metric-value">${total_cost:.4f}</div>
                    </div>
                </div>
            """
            st.markdown(metrics_html, unsafe_allow_html=True)
            
            # Performance Visualization Section
            st.markdown("## üìà Performance Analysis")
            col1, col2 = st.columns(2)
            
            with col1:
                st.plotly_chart(
                    create_success_rate_chart(model_performance),
                    use_container_width=True
                )
            
            with col2:
                st.plotly_chart(
                    create_cost_analysis_chart(model_performance),
                    use_container_width=True
                )
            
            # Model Performance Table
            st.markdown("## üîç Model Performance Details")
            performance_df = pd.DataFrame([
                {
                    'Model': model,
                    'Success Rate (%)': f"{perf.success_rate:.1f}",
                    'Total Tests': perf.total_tests,
                    'Total Cost ($)': f"{perf.total_cost:.4f}",
                    'Avg Latency (ms)': f"{perf.avg_latency:.1f}"
                }
                for model, perf in model_performance.items()
            ])
            
            st.dataframe(
                performance_df.set_index('Model'),
                use_container_width=True,
                height=150
            )
            
            # Error Analysis Section
            st.markdown("## ‚ùå Error Analysis")
            if error_patterns:
                for pattern in error_patterns:
                    st.markdown(f"""
                        <div class="card">
                            <h4>Error Pattern (Occurred {pattern['frequency']} times)</h4>
                            <p><strong>Error Message:</strong> {pattern['error']}</p>
                            <p><strong>Affected Models:</strong> {', '.join(pattern['affected_models'])}</p>
                            <p><strong>Example Variables:</strong></p>
                            <ul>
                                {''.join([f"<li>{var}: {value}</li>" for var, value in pattern['example_vars'].items()])}
                            </ul>
                        </div>
                    """, unsafe_allow_html=True)
            else:
                st.info("No significant error patterns found.")
            
            # Recommendations Section
            st.markdown("## üìã Recommendations")
            for rec in recommendations:
                severity_class = f"recommendation-{rec['severity'].lower()}"
                st.markdown(f"""
                    <div class="card {severity_class}">
                        <h4>[{rec['severity']}] {rec['category']}</h4>
                        <p><strong>Finding:</strong> {rec['finding']}</p>
                        <p><strong>Impact:</strong> {rec['impact']}</p>
                        <p><strong>Recommended Actions:</strong></p>
                        <ul>
                            {''.join([f"<li>{action}</li>" for action in rec['actions']])}
                        </ul>
                    </div>
                """, unsafe_allow_html=True)
            
            # Cleanup temporary file
            os.remove("temp_results.json")
            
        except Exception as e:
            st.error(f"Error analyzing file: {str(e)}")
            st.error("Please make sure you've uploaded a valid promptfoo results file.")

if __name__ == "__main__":
    main() 
